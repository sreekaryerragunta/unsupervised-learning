# Gaussian Mixture Models - Theory

## What is GMM?

**Gaussian Mixture Model** - a probabilistic clustering model that assumes data is generated from a mixture of Gaussian distributions with unknown parameters.

**Core idea**: Model data as coming from K Gaussian distributions (components), each with its own mean, covariance, and mixing weight. Soft clustering where points have probability of belonging to each cluster.

---

## Probabilistic Framework

### Mixture Model

Data point $x$ is generated by:
1. Select component $k$ with probability $\pi_k$ (mixing coefficient)
2. Sample $x$ from Gaussian distribution $N(\mu_k, \Sigma_k)$

**Probability of observing $x$**:
$$p(x) = \sum_{k=1}^{K}\pi_k \cdot N(x|\mu_k, \Sigma_k)$$

Where:
- $\pi_k$: Mixing coefficient (weight) for component $k$, $\sum_k \pi_k = 1$
- $\mu_k$: Mean vector of component $k$
- $\Sigma_k$: Covariance matrix of component $k$
- $N(x|\mu_k, \Sigma_k)$: Multivariate Gaussian PDF

---

## Expectation-Maximization (EM) Algorithm

**Problem**: Given data, estimate $\theta = \{\pi_k, \mu_k, \Sigma_k\}$ for all components

**Solution**: EM algorithm alternates between two steps:

### E-Step (Expectation)

Compute responsibility $\gamma_{nk}$ - probability that point $n$ belongs to component $k$:

$$\gamma_{nk} = \frac{\pi_k \cdot N(x_n|\mu_k, \Sigma_k)}{\sum_{j=1}^{K}\pi_j \cdot N(x_n|\mu_j, \Sigma_j)}$$

**Interpretation**: Soft assignment - each point partially belongs to all clusters

### M-Step (Maximization)

Update parameters using weighted maximum likelihood:

**Mixing coefficients**:
$$\pi_k = \frac{N_k}{N}$$ 
where $N_k = \sum_{n=1}^{N}\gamma_{nk}$

**Means**:
$$\mu_k = \frac{1}{N_k}\sum_{n=1}^{N}\gamma_{nk}x_n$$

**Covariances**:
$$\Sigma_k = \frac{1}{N_k}\sum_{n=1}^{N}\gamma_{nk}(x_n - \mu_k)(x_n - \mu_k)^T$$

**Iterate** until convergence (log-likelihood stops improving)

---

## Hard vs Soft Clustering

### K-Means (Hard Clustering)
- Each point belongs to exactly one cluster
- Binary assignment: 0 or 1
- Decision: Closest centroid

### GMM (Soft Clustering)
- Each point has probability of belonging to each cluster
- Continuous assignment: [0, 1]
- Decision: Weighted by probability
- More flexible, captures uncertainty

---

## Covariance Types

Control shape of Gaussian components:

### 1. Full Covariance
$$\Sigma_k = \text{full matrix (n Ã— n)}$$
- Most flexible
- Ellipsoids with any orientation
- Many parameters: $O(n^2)$ per component
- Requires more data

### 2. Diagonal Covariance
$$\Sigma_k = \text{diag}(\sigma_{k1}^2, ..., \sigma_{kn}^2)$$
- Axis-aligned ellipsoids
- Fewer parameters: $O(n)$ per component
- Assumes features independent within cluster

### 3. Spherical Covariance
$$\Sigma_k = \sigma_k^2 I$$
- Circular/spherical clusters
- Fewest parameters: $O(1)$ per component
- Similar to K-Means

### 4. Tied Covariance
$$\Sigma_1 = \Sigma_2 = ... = \Sigma_K$$
- Same shape for all components
- Reduces parameters
- Assumes similar cluster shapes

---

## Model Selection

### Number of Components (K)

**Methods**:

1. **BIC (Bayesian Information Criterion)**:
   $$BIC = -2\ln(L) + p\ln(N)$$
   - Lower is better
   - Penalizes complexity
   - Most commonly used

2. **AIC (Akaike Information Criterion)**:
   $$AIC = -2\ln(L) + 2p$$
   - Lower is better
   - Less penalty than BIC
   - Tends to select more components

3. **Cross-Validation**:
   - Hold out data
   - Evaluate likelihood on validation set

**Practical**: Try range of K values, plot BIC/AIC, choose elbow

---

## Initialization

**Critical for EM convergence**

### K-Means Initialization (Default)
1. Run K-Means on data
2. Use cluster centers as initial $\mu_k$
3. Compute sample covariance for each cluster as $\Sigma_k$
4. Use cluster proportions as $\pi_k$

**Advantages**:
- Better than random
- Fast
- Usually good results

### Random Initialization
- Randomly select K points as means
- Identity or sample covariance for $\Sigma_k$
- Uniform $\pi_k$

**Disadvantage**: May converge to poor local optimum

---

## GMM vs K-Means

| Aspect | GMM | K-Means |
|--------|-----|---------|
| **Assignment** | Soft (probabilistic) | Hard (deterministic) |
| **Cluster shape** | Elliptical (any) | Spherical |
| **Parameters** | Mean, covariance, weight | Only centroids |
| **Flexibility** | High | Low |
| **Outliers** | Probabilistic handling | Sensitive |
| **Speed** | Slower (EM) | Faster |
| **Assumptions** | Gaussian data | Euclidean clusters |

---

## Advantages

1. **Soft clustering**: Captures uncertainty
2. **Flexible shapes**: Elliptical clusters with any orientation
3. **Probabilistic**: Provides uncertainty estimates
4. **Generative model**: Can sample new data
5. **Principled**: Maximum likelihood framework
6. **Density estimation**: Models data distribution

---

## Disadvantages

1. **Sensitive to initialization**: Multiple runs needed
2. **Local optima**: EM finds local, not global optimum
3. **Assumes Gaussian**: Fails if data isn't Gaussian
4. **Computationally expensive**: Slower than K-Means
5. **Overfitting risk**: Many parameters with full covariance
6. **Requires K**: Must specify number of components

---

## Practical Applications

### Anomaly Detection
- Low probability points = anomalies
- Use likelihood threshold

### Density Estimation
- Model complex distributions
- Mixture of simples = complex

### Semi-Supervised Learning
- Use labeled data to guide component assignment
- Soft labels for unlabeled data

### Image Segmentation
- Pixel colors as mixture
- Each component = region

---

## Practical Tips

### Preprocessing:
1. **Scale features**: Critical for covariance estimation
2. **Remove outliers**: Can create singleton components
3. **Handle missing**: Impute before GMM

### Parameter Selection:
1. **Start with K-Means**: Use result for initialization
2. **Try multiple K**: Use BIC to select
3. **Covariance type**: 
   - Full: Sufficient data, want flexibility
   - Diagonal: High dimensions, limited data
   - Spherical: Similar to K-Means
4. **Multiple runs**: EM sensitive to initialization

### Convergence:
1. **Set max iterations**: Prevent infinite loops
2. **Tolerance**: Stop when log-likelihood change < threshold
3. **Monitor**: Track log-likelihood over iterations

### Common Issues:
1. **Singular covariance**: Add small regularization (1e-6 to diagonal)
2. **Empty components**: Reinitialize or reduce K
3. **One dominant component**: Increase K or change initialization
4. **Slow convergence**: Better initialization or different covariance type

---

## Extensions

### Bayesian GMM
- Place priors on parameters
- Automatic model selection
- Variational inference

### Infinite GMM (Dirichlet Process)
- Automatically determine K
- Non-parametric Bayesian approach

### Robust GMM
- Student-t mixture for heavy tails
- Better with outliers

---

## Key Questions

**Q: When to use GMM vs K-Means?**
A: Use GMM when you need soft assignments, elliptical clusters, or probability estimates. Use K-Means for speed, simplicity, or when hard assignments suffice.

**Q: How does EM avoid local optima?**
A: It doesn't. Run EM multiple times with different initializations and select the result with highest log-likelihood.

**Q: What if my data isn't Gaussian?**
A: GMM may still work as approximation (mixtures can fit complex distributions), but consider alternatives like kernel density estimation or DBSCAN for arbitrary shapes.

**Q: How to choose covariance type?**
A: Full for flexibility (needs more data), diagonal for high dimensions or moderate data, spherical when K-Means works. Use BIC to compare.

---

**Next**: See implementation for EM algorithm and model selection demonstrations.
